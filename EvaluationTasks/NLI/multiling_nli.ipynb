{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc5759-d6a9-4e7e-8f61-fcc84b69a8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdae2e7c-370e-407c-a87a-2e61f782f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 06:39:16.257815: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-11 06:39:16.282131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-11 06:39:16.718589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels: {0, 1, 2}\n",
      "Number of Classes: 3\n",
      "faisalq/bert-base-arabic-wordpiece, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/bert-base-arabic-wordpiece and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3290' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3290/3290 21:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.008500</td>\n",
       "      <td>0.909323</td>\n",
       "      <td>0.575714</td>\n",
       "      <td>0.549143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.826953</td>\n",
       "      <td>0.626810</td>\n",
       "      <td>0.611991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.816800</td>\n",
       "      <td>0.836469</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.628004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>0.795567</td>\n",
       "      <td>0.649571</td>\n",
       "      <td>0.634084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.727900</td>\n",
       "      <td>0.794047</td>\n",
       "      <td>0.653143</td>\n",
       "      <td>0.634771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.648600</td>\n",
       "      <td>0.862447</td>\n",
       "      <td>0.662286</td>\n",
       "      <td>0.651840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>0.853524</td>\n",
       "      <td>0.666810</td>\n",
       "      <td>0.654346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.573600</td>\n",
       "      <td>0.839994</td>\n",
       "      <td>0.664571</td>\n",
       "      <td>0.658136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.440600</td>\n",
       "      <td>0.982343</td>\n",
       "      <td>0.661762</td>\n",
       "      <td>0.653850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.423300</td>\n",
       "      <td>0.953802</td>\n",
       "      <td>0.665667</td>\n",
       "      <td>0.657209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.996178</td>\n",
       "      <td>0.667810</td>\n",
       "      <td>0.657964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>1.105157</td>\n",
       "      <td>0.654810</td>\n",
       "      <td>0.644074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>1.165460</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.646753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>1.279092</td>\n",
       "      <td>0.656333</td>\n",
       "      <td>0.649433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.202400</td>\n",
       "      <td>1.269608</td>\n",
       "      <td>0.658619</td>\n",
       "      <td>0.650224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>1.319805</td>\n",
       "      <td>0.655429</td>\n",
       "      <td>0.645302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>1.406279</td>\n",
       "      <td>0.658381</td>\n",
       "      <td>0.650825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>1.408378</td>\n",
       "      <td>0.657905</td>\n",
       "      <td>0.649263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>1.424986</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.650858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>1.577399</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.646351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>1.553002</td>\n",
       "      <td>0.659143</td>\n",
       "      <td>0.648530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>1.588172</td>\n",
       "      <td>0.658238</td>\n",
       "      <td>0.648255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>1.661921</td>\n",
       "      <td>0.655048</td>\n",
       "      <td>0.647323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>1.650506</td>\n",
       "      <td>0.656143</td>\n",
       "      <td>0.648459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.073500</td>\n",
       "      <td>1.715616</td>\n",
       "      <td>0.655333</td>\n",
       "      <td>0.645854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>1.741351</td>\n",
       "      <td>0.658571</td>\n",
       "      <td>0.649119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>1.730165</td>\n",
       "      <td>0.658476</td>\n",
       "      <td>0.649820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/bert-base-arabic-wordpiece, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/bert-base-arabic-wordpiece and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3290' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3290/3290 21:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.012800</td>\n",
       "      <td>0.906327</td>\n",
       "      <td>0.567905</td>\n",
       "      <td>0.549694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.887100</td>\n",
       "      <td>0.840768</td>\n",
       "      <td>0.620333</td>\n",
       "      <td>0.603527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.815700</td>\n",
       "      <td>0.836209</td>\n",
       "      <td>0.642810</td>\n",
       "      <td>0.626970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.730900</td>\n",
       "      <td>0.791345</td>\n",
       "      <td>0.652905</td>\n",
       "      <td>0.639252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.722600</td>\n",
       "      <td>0.776877</td>\n",
       "      <td>0.660286</td>\n",
       "      <td>0.645746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.869664</td>\n",
       "      <td>0.662381</td>\n",
       "      <td>0.650303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.558700</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>0.661714</td>\n",
       "      <td>0.651973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.556100</td>\n",
       "      <td>0.853379</td>\n",
       "      <td>0.664190</td>\n",
       "      <td>0.656413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.427600</td>\n",
       "      <td>1.016338</td>\n",
       "      <td>0.659810</td>\n",
       "      <td>0.650888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>0.974030</td>\n",
       "      <td>0.666143</td>\n",
       "      <td>0.657315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>1.014735</td>\n",
       "      <td>0.666571</td>\n",
       "      <td>0.656893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>1.127641</td>\n",
       "      <td>0.658762</td>\n",
       "      <td>0.650084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>1.148300</td>\n",
       "      <td>0.660952</td>\n",
       "      <td>0.650022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>1.266685</td>\n",
       "      <td>0.658619</td>\n",
       "      <td>0.652215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>1.280390</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.647711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>1.314205</td>\n",
       "      <td>0.655619</td>\n",
       "      <td>0.648235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>1.434773</td>\n",
       "      <td>0.654429</td>\n",
       "      <td>0.646323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>1.453100</td>\n",
       "      <td>0.655667</td>\n",
       "      <td>0.647655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>1.448352</td>\n",
       "      <td>0.657762</td>\n",
       "      <td>0.650831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>1.580127</td>\n",
       "      <td>0.647571</td>\n",
       "      <td>0.639601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>1.575170</td>\n",
       "      <td>0.659571</td>\n",
       "      <td>0.652013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>1.592876</td>\n",
       "      <td>0.659190</td>\n",
       "      <td>0.650695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>1.682978</td>\n",
       "      <td>0.655524</td>\n",
       "      <td>0.648004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>1.714447</td>\n",
       "      <td>0.655571</td>\n",
       "      <td>0.647185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>1.727728</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.648024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>1.736277</td>\n",
       "      <td>0.658619</td>\n",
       "      <td>0.650375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>1.750712</td>\n",
       "      <td>0.657429</td>\n",
       "      <td>0.650238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/bert-base-arabic-wordpiece, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/bert-base-arabic-wordpiece and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3290' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3290/3290 21:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.012800</td>\n",
       "      <td>0.906327</td>\n",
       "      <td>0.567905</td>\n",
       "      <td>0.549694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.887100</td>\n",
       "      <td>0.840768</td>\n",
       "      <td>0.620333</td>\n",
       "      <td>0.603527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.815700</td>\n",
       "      <td>0.836209</td>\n",
       "      <td>0.642810</td>\n",
       "      <td>0.626970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.730900</td>\n",
       "      <td>0.791345</td>\n",
       "      <td>0.652905</td>\n",
       "      <td>0.639252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.722600</td>\n",
       "      <td>0.776877</td>\n",
       "      <td>0.660286</td>\n",
       "      <td>0.645746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.869664</td>\n",
       "      <td>0.662381</td>\n",
       "      <td>0.650303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.558700</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>0.661714</td>\n",
       "      <td>0.651973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.556100</td>\n",
       "      <td>0.853379</td>\n",
       "      <td>0.664190</td>\n",
       "      <td>0.656413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.427600</td>\n",
       "      <td>1.016338</td>\n",
       "      <td>0.659810</td>\n",
       "      <td>0.650888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>0.974030</td>\n",
       "      <td>0.666143</td>\n",
       "      <td>0.657315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>1.014735</td>\n",
       "      <td>0.666571</td>\n",
       "      <td>0.656893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>1.127641</td>\n",
       "      <td>0.658762</td>\n",
       "      <td>0.650084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>1.148300</td>\n",
       "      <td>0.660952</td>\n",
       "      <td>0.650022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>1.266685</td>\n",
       "      <td>0.658619</td>\n",
       "      <td>0.652215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>1.280390</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.647711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>1.314205</td>\n",
       "      <td>0.655619</td>\n",
       "      <td>0.648235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>1.434773</td>\n",
       "      <td>0.654429</td>\n",
       "      <td>0.646323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>1.453100</td>\n",
       "      <td>0.655667</td>\n",
       "      <td>0.647655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>1.448352</td>\n",
       "      <td>0.657762</td>\n",
       "      <td>0.650831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>1.580127</td>\n",
       "      <td>0.647571</td>\n",
       "      <td>0.639601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>1.575170</td>\n",
       "      <td>0.659571</td>\n",
       "      <td>0.652013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>1.592876</td>\n",
       "      <td>0.659190</td>\n",
       "      <td>0.650695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>1.682978</td>\n",
       "      <td>0.655524</td>\n",
       "      <td>0.648004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>1.714447</td>\n",
       "      <td>0.655571</td>\n",
       "      <td>0.647185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>1.727728</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.648024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>1.736277</td>\n",
       "      <td>0.658619</td>\n",
       "      <td>0.650375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>1.750712</td>\n",
       "      <td>0.657429</td>\n",
       "      <td>0.650238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/bert-base-arabic-senpiece, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/bert-base-arabic-senpiece and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3290' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3290/3290 21:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>0.873665</td>\n",
       "      <td>0.599190</td>\n",
       "      <td>0.580753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.828511</td>\n",
       "      <td>0.631810</td>\n",
       "      <td>0.617675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.799252</td>\n",
       "      <td>0.652952</td>\n",
       "      <td>0.636988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>0.805312</td>\n",
       "      <td>0.658190</td>\n",
       "      <td>0.644195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.699200</td>\n",
       "      <td>0.784036</td>\n",
       "      <td>0.662048</td>\n",
       "      <td>0.646201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.608500</td>\n",
       "      <td>0.823329</td>\n",
       "      <td>0.670857</td>\n",
       "      <td>0.658548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.828429</td>\n",
       "      <td>0.668619</td>\n",
       "      <td>0.658897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.825902</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>0.660694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.403100</td>\n",
       "      <td>0.982212</td>\n",
       "      <td>0.662667</td>\n",
       "      <td>0.655008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>1.013230</td>\n",
       "      <td>0.659238</td>\n",
       "      <td>0.652079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.979258</td>\n",
       "      <td>0.665190</td>\n",
       "      <td>0.656572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>1.112515</td>\n",
       "      <td>0.657333</td>\n",
       "      <td>0.649728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>1.114783</td>\n",
       "      <td>0.665333</td>\n",
       "      <td>0.658720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.237800</td>\n",
       "      <td>1.277958</td>\n",
       "      <td>0.664048</td>\n",
       "      <td>0.654504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>1.308656</td>\n",
       "      <td>0.663286</td>\n",
       "      <td>0.654902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>1.331392</td>\n",
       "      <td>0.659333</td>\n",
       "      <td>0.652765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>1.418339</td>\n",
       "      <td>0.666619</td>\n",
       "      <td>0.658132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>1.492586</td>\n",
       "      <td>0.660381</td>\n",
       "      <td>0.654079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>1.446907</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.657334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>1.590630</td>\n",
       "      <td>0.664857</td>\n",
       "      <td>0.657006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>1.618957</td>\n",
       "      <td>0.665524</td>\n",
       "      <td>0.656461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>1.611215</td>\n",
       "      <td>0.664714</td>\n",
       "      <td>0.657760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>1.730041</td>\n",
       "      <td>0.663905</td>\n",
       "      <td>0.654898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>1.726241</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.661311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>1.762686</td>\n",
       "      <td>0.665095</td>\n",
       "      <td>0.657346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>1.785526</td>\n",
       "      <td>0.663381</td>\n",
       "      <td>0.656514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>1.777955</td>\n",
       "      <td>0.664190</td>\n",
       "      <td>0.656852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/bert-base-arabic-senpiece, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/bert-base-arabic-senpiece and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3290' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3290/3290 21:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>0.873665</td>\n",
       "      <td>0.599190</td>\n",
       "      <td>0.580753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.828511</td>\n",
       "      <td>0.631810</td>\n",
       "      <td>0.617675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.799252</td>\n",
       "      <td>0.652952</td>\n",
       "      <td>0.636988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>0.805312</td>\n",
       "      <td>0.658190</td>\n",
       "      <td>0.644195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.699200</td>\n",
       "      <td>0.784036</td>\n",
       "      <td>0.662048</td>\n",
       "      <td>0.646201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.608500</td>\n",
       "      <td>0.823329</td>\n",
       "      <td>0.670857</td>\n",
       "      <td>0.658548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.828429</td>\n",
       "      <td>0.668619</td>\n",
       "      <td>0.658897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.825902</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>0.660694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.403100</td>\n",
       "      <td>0.982212</td>\n",
       "      <td>0.662667</td>\n",
       "      <td>0.655008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>1.013230</td>\n",
       "      <td>0.659238</td>\n",
       "      <td>0.652079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.979258</td>\n",
       "      <td>0.665190</td>\n",
       "      <td>0.656572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>1.112515</td>\n",
       "      <td>0.657333</td>\n",
       "      <td>0.649728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>1.114783</td>\n",
       "      <td>0.665333</td>\n",
       "      <td>0.658720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.237800</td>\n",
       "      <td>1.277958</td>\n",
       "      <td>0.664048</td>\n",
       "      <td>0.654504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>1.308656</td>\n",
       "      <td>0.663286</td>\n",
       "      <td>0.654902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>1.331392</td>\n",
       "      <td>0.659333</td>\n",
       "      <td>0.652765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>1.418339</td>\n",
       "      <td>0.666619</td>\n",
       "      <td>0.658132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>1.492586</td>\n",
       "      <td>0.660381</td>\n",
       "      <td>0.654079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>1.446907</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.657334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>1.590630</td>\n",
       "      <td>0.664857</td>\n",
       "      <td>0.657006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>1.618957</td>\n",
       "      <td>0.665524</td>\n",
       "      <td>0.656461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>1.611215</td>\n",
       "      <td>0.664714</td>\n",
       "      <td>0.657760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>1.730041</td>\n",
       "      <td>0.663905</td>\n",
       "      <td>0.654898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>1.726241</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.661311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>1.762686</td>\n",
       "      <td>0.665095</td>\n",
       "      <td>0.657346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>1.785526</td>\n",
       "      <td>0.663381</td>\n",
       "      <td>0.656514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>1.777955</td>\n",
       "      <td>0.664190</td>\n",
       "      <td>0.656852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/bert-base-arabic-senpiece, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/bert-base-arabic-senpiece and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3290' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3290/3290 21:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>0.873665</td>\n",
       "      <td>0.599190</td>\n",
       "      <td>0.580753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.828511</td>\n",
       "      <td>0.631810</td>\n",
       "      <td>0.617675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.799252</td>\n",
       "      <td>0.652952</td>\n",
       "      <td>0.636988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>0.805312</td>\n",
       "      <td>0.658190</td>\n",
       "      <td>0.644195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.699200</td>\n",
       "      <td>0.784036</td>\n",
       "      <td>0.662048</td>\n",
       "      <td>0.646201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.608500</td>\n",
       "      <td>0.823329</td>\n",
       "      <td>0.670857</td>\n",
       "      <td>0.658548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.828429</td>\n",
       "      <td>0.668619</td>\n",
       "      <td>0.658897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.825902</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>0.660694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.403100</td>\n",
       "      <td>0.982212</td>\n",
       "      <td>0.662667</td>\n",
       "      <td>0.655008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>1.013230</td>\n",
       "      <td>0.659238</td>\n",
       "      <td>0.652079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.979258</td>\n",
       "      <td>0.665190</td>\n",
       "      <td>0.656572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>1.112515</td>\n",
       "      <td>0.657333</td>\n",
       "      <td>0.649728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>1.114783</td>\n",
       "      <td>0.665333</td>\n",
       "      <td>0.658720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.237800</td>\n",
       "      <td>1.277958</td>\n",
       "      <td>0.664048</td>\n",
       "      <td>0.654504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>1.308656</td>\n",
       "      <td>0.663286</td>\n",
       "      <td>0.654902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>1.331392</td>\n",
       "      <td>0.659333</td>\n",
       "      <td>0.652765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>1.418339</td>\n",
       "      <td>0.666619</td>\n",
       "      <td>0.658132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>1.492586</td>\n",
       "      <td>0.660381</td>\n",
       "      <td>0.654079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>1.446907</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.657334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>1.590630</td>\n",
       "      <td>0.664857</td>\n",
       "      <td>0.657006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>1.618957</td>\n",
       "      <td>0.665524</td>\n",
       "      <td>0.656461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>1.611215</td>\n",
       "      <td>0.664714</td>\n",
       "      <td>0.657760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>1.730041</td>\n",
       "      <td>0.663905</td>\n",
       "      <td>0.654898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>1.726241</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.661311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>1.762686</td>\n",
       "      <td>0.665095</td>\n",
       "      <td>0.657346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>1.785526</td>\n",
       "      <td>0.663381</td>\n",
       "      <td>0.656514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>1.777955</td>\n",
       "      <td>0.664190</td>\n",
       "      <td>0.656852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/bert-base-arabic-bbpe, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/bert-base-arabic-bbpe and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3290' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3290/3290 20:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.992300</td>\n",
       "      <td>0.891256</td>\n",
       "      <td>0.580333</td>\n",
       "      <td>0.567090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.830963</td>\n",
       "      <td>0.619619</td>\n",
       "      <td>0.605763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.805600</td>\n",
       "      <td>0.821265</td>\n",
       "      <td>0.636048</td>\n",
       "      <td>0.621276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.809452</td>\n",
       "      <td>0.644810</td>\n",
       "      <td>0.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.805529</td>\n",
       "      <td>0.647048</td>\n",
       "      <td>0.636318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>0.868566</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.643421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.526400</td>\n",
       "      <td>0.879853</td>\n",
       "      <td>0.647905</td>\n",
       "      <td>0.642287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.866407</td>\n",
       "      <td>0.651619</td>\n",
       "      <td>0.642910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>1.044992</td>\n",
       "      <td>0.645810</td>\n",
       "      <td>0.639966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.981877</td>\n",
       "      <td>0.654619</td>\n",
       "      <td>0.647436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>1.027804</td>\n",
       "      <td>0.650571</td>\n",
       "      <td>0.642126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.238300</td>\n",
       "      <td>1.213292</td>\n",
       "      <td>0.643286</td>\n",
       "      <td>0.636009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>1.224622</td>\n",
       "      <td>0.644762</td>\n",
       "      <td>0.637421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.221100</td>\n",
       "      <td>1.362038</td>\n",
       "      <td>0.649143</td>\n",
       "      <td>0.641710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>1.396618</td>\n",
       "      <td>0.642238</td>\n",
       "      <td>0.633897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>1.387459</td>\n",
       "      <td>0.646333</td>\n",
       "      <td>0.639779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.134800</td>\n",
       "      <td>1.504213</td>\n",
       "      <td>0.646905</td>\n",
       "      <td>0.640467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>1.546764</td>\n",
       "      <td>0.645476</td>\n",
       "      <td>0.637727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>1.611268</td>\n",
       "      <td>0.642381</td>\n",
       "      <td>0.636637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>1.681383</td>\n",
       "      <td>0.640190</td>\n",
       "      <td>0.633525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>1.702702</td>\n",
       "      <td>0.647810</td>\n",
       "      <td>0.640159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>1.708016</td>\n",
       "      <td>0.649333</td>\n",
       "      <td>0.640599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>1.816229</td>\n",
       "      <td>0.646524</td>\n",
       "      <td>0.638124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>1.814567</td>\n",
       "      <td>0.647762</td>\n",
       "      <td>0.641595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>1.852801</td>\n",
       "      <td>0.647952</td>\n",
       "      <td>0.640578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>1.882507</td>\n",
       "      <td>0.651381</td>\n",
       "      <td>0.644057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>1.895388</td>\n",
       "      <td>0.650381</td>\n",
       "      <td>0.643373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/bert-base-arabic-bbpe, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/bert-base-arabic-bbpe and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3290' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3290/3290 20:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.992300</td>\n",
       "      <td>0.891256</td>\n",
       "      <td>0.580333</td>\n",
       "      <td>0.567090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.830963</td>\n",
       "      <td>0.619619</td>\n",
       "      <td>0.605763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.805600</td>\n",
       "      <td>0.821265</td>\n",
       "      <td>0.636048</td>\n",
       "      <td>0.621276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.809452</td>\n",
       "      <td>0.644810</td>\n",
       "      <td>0.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.805529</td>\n",
       "      <td>0.647048</td>\n",
       "      <td>0.636318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>0.868566</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.643421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.526400</td>\n",
       "      <td>0.879853</td>\n",
       "      <td>0.647905</td>\n",
       "      <td>0.642287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.866407</td>\n",
       "      <td>0.651619</td>\n",
       "      <td>0.642910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>1.044992</td>\n",
       "      <td>0.645810</td>\n",
       "      <td>0.639966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.981877</td>\n",
       "      <td>0.654619</td>\n",
       "      <td>0.647436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>1.027804</td>\n",
       "      <td>0.650571</td>\n",
       "      <td>0.642126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.238300</td>\n",
       "      <td>1.213292</td>\n",
       "      <td>0.643286</td>\n",
       "      <td>0.636009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>1.224622</td>\n",
       "      <td>0.644762</td>\n",
       "      <td>0.637421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.221100</td>\n",
       "      <td>1.362038</td>\n",
       "      <td>0.649143</td>\n",
       "      <td>0.641710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>1.396618</td>\n",
       "      <td>0.642238</td>\n",
       "      <td>0.633897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>1.387459</td>\n",
       "      <td>0.646333</td>\n",
       "      <td>0.639779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.134800</td>\n",
       "      <td>1.504213</td>\n",
       "      <td>0.646905</td>\n",
       "      <td>0.640467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>1.546764</td>\n",
       "      <td>0.645476</td>\n",
       "      <td>0.637727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>1.611268</td>\n",
       "      <td>0.642381</td>\n",
       "      <td>0.636637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>1.681383</td>\n",
       "      <td>0.640190</td>\n",
       "      <td>0.633525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>1.702702</td>\n",
       "      <td>0.647810</td>\n",
       "      <td>0.640159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>1.708016</td>\n",
       "      <td>0.649333</td>\n",
       "      <td>0.640599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>1.816229</td>\n",
       "      <td>0.646524</td>\n",
       "      <td>0.638124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>1.814567</td>\n",
       "      <td>0.647762</td>\n",
       "      <td>0.641595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>1.852801</td>\n",
       "      <td>0.647952</td>\n",
       "      <td>0.640578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>1.882507</td>\n",
       "      <td>0.651381</td>\n",
       "      <td>0.644057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>1.895388</td>\n",
       "      <td>0.650381</td>\n",
       "      <td>0.643373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/bert-base-arabic-bbpe, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/bert-base-arabic-bbpe and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3290' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3290/3290 20:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.992300</td>\n",
       "      <td>0.891256</td>\n",
       "      <td>0.580333</td>\n",
       "      <td>0.567090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.830963</td>\n",
       "      <td>0.619619</td>\n",
       "      <td>0.605763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.805600</td>\n",
       "      <td>0.821265</td>\n",
       "      <td>0.636048</td>\n",
       "      <td>0.621276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.809452</td>\n",
       "      <td>0.644810</td>\n",
       "      <td>0.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.805529</td>\n",
       "      <td>0.647048</td>\n",
       "      <td>0.636318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>0.868566</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.643421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.526400</td>\n",
       "      <td>0.879853</td>\n",
       "      <td>0.647905</td>\n",
       "      <td>0.642287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.866407</td>\n",
       "      <td>0.651619</td>\n",
       "      <td>0.642910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>1.044992</td>\n",
       "      <td>0.645810</td>\n",
       "      <td>0.639966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.981877</td>\n",
       "      <td>0.654619</td>\n",
       "      <td>0.647436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>1.027804</td>\n",
       "      <td>0.650571</td>\n",
       "      <td>0.642126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.238300</td>\n",
       "      <td>1.213292</td>\n",
       "      <td>0.643286</td>\n",
       "      <td>0.636009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>1.224622</td>\n",
       "      <td>0.644762</td>\n",
       "      <td>0.637421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.221100</td>\n",
       "      <td>1.362038</td>\n",
       "      <td>0.649143</td>\n",
       "      <td>0.641710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>1.396618</td>\n",
       "      <td>0.642238</td>\n",
       "      <td>0.633897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>1.387459</td>\n",
       "      <td>0.646333</td>\n",
       "      <td>0.639779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.134800</td>\n",
       "      <td>1.504213</td>\n",
       "      <td>0.646905</td>\n",
       "      <td>0.640467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>1.546764</td>\n",
       "      <td>0.645476</td>\n",
       "      <td>0.637727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>1.611268</td>\n",
       "      <td>0.642381</td>\n",
       "      <td>0.636637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>1.681383</td>\n",
       "      <td>0.640190</td>\n",
       "      <td>0.633525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>1.702702</td>\n",
       "      <td>0.647810</td>\n",
       "      <td>0.640159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>1.708016</td>\n",
       "      <td>0.649333</td>\n",
       "      <td>0.640599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>1.816229</td>\n",
       "      <td>0.646524</td>\n",
       "      <td>0.638124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>1.814567</td>\n",
       "      <td>0.647762</td>\n",
       "      <td>0.641595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>1.852801</td>\n",
       "      <td>0.647952</td>\n",
       "      <td>0.640578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>1.882507</td>\n",
       "      <td>0.651381</td>\n",
       "      <td>0.644057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>1.895388</td>\n",
       "      <td>0.650381</td>\n",
       "      <td>0.643373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faisalq/bert-base-arabic-bbpe</td>\n",
       "      <td>0.654619</td>\n",
       "      <td>0.647436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faisalq/bert-base-arabic-bbpe</td>\n",
       "      <td>0.654619</td>\n",
       "      <td>0.647436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>faisalq/bert-base-arabic-bbpe</td>\n",
       "      <td>0.654619</td>\n",
       "      <td>0.647436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>faisalq/bert-base-arabic-senpiece</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.661311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faisalq/bert-base-arabic-senpiece</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.661311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>faisalq/bert-base-arabic-senpiece</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.661311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>faisalq/bert-base-arabic-wordpiece</td>\n",
       "      <td>0.664571</td>\n",
       "      <td>0.658136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model  Accuracy        F1\n",
       "0       faisalq/bert-base-arabic-bbpe  0.654619  0.647436\n",
       "1       faisalq/bert-base-arabic-bbpe  0.654619  0.647436\n",
       "2       faisalq/bert-base-arabic-bbpe  0.654619  0.647436\n",
       "3   faisalq/bert-base-arabic-senpiece  0.668667  0.661311\n",
       "4   faisalq/bert-base-arabic-senpiece  0.668667  0.661311\n",
       "5   faisalq/bert-base-arabic-senpiece  0.668667  0.661311\n",
       "6  faisalq/bert-base-arabic-wordpiece  0.664571  0.658136"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "log_file = 'ml_nli.txt'\n",
    "\n",
    "# ds1 = load_dataset('MoritzLaurer/multilingual-NLI-26lang-2mil7', split='ar_anli') \n",
    "# ds2 = load_dataset('MoritzLaurer/multilingual-NLI-26lang-2mil7', split='ar_fever')\n",
    "# ds3 = load_dataset('MoritzLaurer/multilingual-NLI-26lang-2mil7', split='ar_ling')\n",
    "# ds4 = load_dataset('MoritzLaurer/multilingual-NLI-26lang-2mil7', split='ar_mnli')\n",
    "# ds5 = load_dataset('MoritzLaurer/multilingual-NLI-26lang-2mil7', split='ar_wanli')\n",
    "# # display(ds)\n",
    "\n",
    "\n",
    "ds1 = pd.read_parquet('multi_nli/ar_anli-00000-of-00001-d5ddcd7a96189c94.parquet')\n",
    "ds2 = pd.read_parquet('multi_nli/ar_fever-00000-of-00001-75e864b8c1cf8d17.parquet')\n",
    "ds3 = pd.read_parquet('multi_nli/ar_ling-00000-of-00001-f4e042f46b091cf7.parquet')\n",
    "ds4 = pd.read_parquet('multi_nli/ar_mnli-00000-of-00001-13deaea9065575d9.parquet')\n",
    "ds5 = pd.read_parquet('multi_nli/ar_wanli-00000-of-00001-7f580e9d2eff0880.parquet')\n",
    "\n",
    "ds1 = Dataset.from_pandas(ds1)\n",
    "ds2 = Dataset.from_pandas(ds2)\n",
    "ds3 = Dataset.from_pandas(ds3)\n",
    "ds4 = Dataset.from_pandas(ds4)\n",
    "ds5 = Dataset.from_pandas(ds5)\n",
    "\n",
    "ds1 = ds1.train_test_split(test_size=0.2)\n",
    "ds2 = ds2.train_test_split(test_size=0.2)\n",
    "ds3 = ds3.train_test_split(test_size=0.2)\n",
    "ds4 = ds4.train_test_split(test_size=0.2)\n",
    "ds5 = ds5.train_test_split(test_size=0.2)\n",
    "\n",
    "# display(ds4)\n",
    "\n",
    "dataset_t = concatenate_datasets([ds1['train'], ds2['train'], ds3['train'], ds4['train'], ds5['train']])\n",
    "dataset_v = concatenate_datasets([ds1['test'], ds2['test'], ds3['test'], ds4['test'], ds5['test']])\n",
    "\n",
    "# display(dataset_train)\n",
    "display(len(dataset_t))\n",
    "display(len(dataset_v))\n",
    "\n",
    "unique_labels = set(dataset_t['label'])\n",
    "classes_num = len(unique_labels)\n",
    "\n",
    "print(f'Unique Labels: {unique_labels}')\n",
    "print(f'Number of Classes: {classes_num}')\n",
    "\n",
    "models = ['faisalq/bert-base-arabic-wordpiece', 'faisalq/bert-base-arabic-senpiece',\n",
    "                'faisalq/bert-base-arabic-bbpe']\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write('Model,Accuracy,F1\\n')\n",
    "\n",
    "for model_name in models:\n",
    "    for i in range(3):\n",
    "        print(f'{model_name}, try:{i}')\n",
    "        dataset_train = dataset_t\n",
    "        dataset_validation = dataset_v\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name,\n",
    "                                                              num_labels=classes_num).to('cuda')                                                 \n",
    "                                                     \n",
    "        \n",
    "        max_length = 128\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding=\"max_length\",\n",
    "                            max_length=max_length)\n",
    "        \n",
    "        \n",
    "        dataset_train = dataset_train.map(preprocess_function, batched=True)\n",
    "        dataset_validation = dataset_validation.map(preprocess_function, batched=True)\n",
    "\n",
    "                \n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            predictions = np.argmax(logits, axis=-1)    \n",
    "            acc = accuracy_score(labels, predictions)        \n",
    "            f1 = f1_score(labels, predictions, average='macro')   \n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f'{model_name},{acc},{f1}\\n')\n",
    "            return {'accuracy': acc, 'f1_score': f1}\n",
    "        \n",
    "\n",
    "        epochs = 10\n",
    "        save_steps = 10000 #save checkpoint every 10000 steps\n",
    "        batch_size = 256\n",
    "        \n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir = 'bert/',\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs = epochs,\n",
    "            per_device_train_batch_size = batch_size,\n",
    "            per_device_eval_batch_size = batch_size,\n",
    "            save_steps = save_steps,\n",
    "            save_total_limit = 1, \n",
    "            fp16=True,\n",
    "            learning_rate = 5e-5,  # 5e-5 is the default\n",
    "            # weight_decay=0.01,\n",
    "            logging_steps = 120, #50_000\n",
    "            evaluation_strategy = 'steps',\n",
    "            eval_steps = 120\n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset_train,\n",
    "            eval_dataset=dataset_validation,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        # trainer.evaluate()\n",
    "\n",
    "\n",
    "results = pd.read_csv(log_file)\n",
    "\n",
    "best_results = results.groupby('Model', as_index=False)['F1'].max()\n",
    "\n",
    "best_results = pd.merge(best_results, results, on=['Model', 'F1'])\n",
    "best_results = best_results[['Model', 'Accuracy', 'F1']]\n",
    "best_results.to_csv('mlnli_results.csv')\n",
    "display(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471d7fd-ffc9-4341-b664-7e0daf3135bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43aa7fb-c909-41ac-a427-e2a499c8df9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c30df8-1cf0-4ad2-bbfe-8b16c80b0c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6149d-6e22-4457-8143-fa393ccd011f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ecbf0-c481-4f21-b7c8-c4d896640e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1de169-cbd8-4f8e-b172-61a5eb8aa60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
