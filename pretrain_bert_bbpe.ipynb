{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12e46f-23c8-4181-9e24-a0276dc12dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy==1.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da4c2c-e801-4fa6-8676-c04e551ee88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.12\n",
    "!pip install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b7f2d-e4a6-4dc1-9a9f-eb0d99dc95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a42c50-df29-4c17-84f2-69da839d31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a49286b-4163-4b95-9c68-fdfe6aaa3dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122dc8fb-4fb8-4065-8231-8cf429b81a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 08:34:23.581089: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-27 08:34:23.687709: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-27 08:34:24.407514: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc47f17-ea91-4dd0-93f8-83eade6763db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec70120-dea9-45ba-841d-fd50f6a46a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/ffq/.cache/huggingface/datasets/text/default-daa5a8d92e7d97fb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275e0112f6574d449bad55f04ca6128a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import glob\n",
    "\n",
    "folder_path = 'text/'\n",
    "files_list = glob.glob(folder_path + \"/*\")\n",
    "dataset = load_dataset('text', data_files=files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce34f548-be5a-4e82-a8bf-c7b4703508b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 127836649\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084954b8-6733-4229-95b4-13e8e6131ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8fb23472b84f9dbd2bd648d5f482a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f729f5f7f6694c248f2558098103cf5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d588eeb481a43cc81e166cb87a449d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2409e109d7574b51a376a22eb1be89ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/ffq/.cache/huggingface/datasets/text/default-daa5a8d92e7d97fb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630bf892252c4345ad9b6dd22ff30a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 12784/12784 [06:02<00:00, 35.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bert_bbpe/tokenizer_config.json',\n",
       " 'bert_bbpe/special_tokens_map.json',\n",
       " 'bert_bbpe/vocab.json',\n",
       " 'bert_bbpe/merges.txt',\n",
       " 'bert_bbpe/added_tokens.json',\n",
       " 'bert_bbpe/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import glob\n",
    "\n",
    "#loading bert tokenizer to work as a base for the new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "folder_path = 'text/'\n",
    "files_list = glob.glob(folder_path + \"/*\")\n",
    "dataset = load_dataset('text', data_files=files_list)\n",
    "\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(dataset['train']), batch_size)):\n",
    "        yield dataset['train'][i: i +batch_size]['text']\n",
    "bert_tokenizer = tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), \n",
    "                                                   vocab_size=50000\n",
    "                                                   #, special_tokens=['[CLS]', '[PAD]','[SEP]','[UNK]','[MASK]']\n",
    "                                                  )\n",
    "bert_tokenizer.save_pretrained('bert_bbpe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae83518-bb71-4be7-8fd0-58e524510c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 127836649\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e8cdcd-050c-4b35-85be-aa8b9ad5cb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 17:33:41.779230: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-22 17:33:41.802559: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-22 17:33:42.326262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Found cached dataset text (/home/ffq/.cache/huggingface/datasets/text/default-daa5a8d92e7d97fb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fae9482cedf4808867a9c33c66d1967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127836649 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "127836649"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell-3\n",
    "#tokenizing the whole text \n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "from datasets import load_dataset\n",
    "import glob\n",
    "import tokenizers\n",
    "from transformers import Trainer, TrainingArguments, LineByLineTextDataset, BertModel\n",
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('faisalq/bert-base-arapoembert')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert_bbpe/')\n",
    "max_seq_length = 128\n",
    "\n",
    "folder_path = 'text/'\n",
    "files_list = glob.glob(folder_path + \"/*\")\n",
    "dataset = load_dataset('text', data_files=files_list)\n",
    "\n",
    "# use to combine files: cat ./* > merged_file\n",
    "\n",
    "def encode_with_truncation(examples):\n",
    "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                   max_length=max_seq_length, return_special_tokens_mask=True)\n",
    "\n",
    "\n",
    "dataset = dataset[\"train\"].map(encode_with_truncation, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# dataset = LineByLineTextDataset(tokenizer = tokenizer, file_path = 'text/text4.txt', \n",
    "#                                block_size = max_seq_length)\n",
    "display(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91036f0-2cdc-444e-a5ab-33c6fe4da9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'special_tokens_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a4de99-d5cf-48a2-b7d0-e187e8acc341",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be026e91-c580-4b44-878b-3a52027c3aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fa0bec-b595-4c65-8e58-cd5060d0d83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/200 shards):   0%|          | 0/127836649 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"tokenized_dataset_bbpe/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2f1916-098c-4603-afa8-cad135901655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 11:21:35.970763: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-26 11:21:36.306548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 11:21:36.881774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#start here\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "from datasets import load_dataset\n",
    "import glob\n",
    "import tokenizers\n",
    "from transformers import Trainer, TrainingArguments, LineByLineTextDataset, BertModel\n",
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_from_disk(\"tokenized_dataset_bbpe/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec45482-b6f0-4281-9f55-829fa6d1eb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124197968"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell-4\n",
    "# model config\n",
    "max_seq_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert_bbpe/')\n",
    "config = BertConfig( vocab_size = 50000, \n",
    "                    hidden_size = 768, \n",
    "                    num_hidden_layers = 12,\n",
    "                    num_attention_heads = 12,\n",
    "                    max_position_embeddings = max_seq_length)\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "display(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adb1308-d307-40a5-af17-6fbd936a5701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1997448' max='1997448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1997448/1997448 60:01:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1205000</td>\n",
       "      <td>2.642800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210000</td>\n",
       "      <td>2.642900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1215000</td>\n",
       "      <td>2.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220000</td>\n",
       "      <td>2.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225000</td>\n",
       "      <td>2.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230000</td>\n",
       "      <td>2.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1235000</td>\n",
       "      <td>2.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240000</td>\n",
       "      <td>2.630300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245000</td>\n",
       "      <td>2.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250000</td>\n",
       "      <td>2.628500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1255000</td>\n",
       "      <td>2.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260000</td>\n",
       "      <td>2.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1265000</td>\n",
       "      <td>2.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270000</td>\n",
       "      <td>2.621200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275000</td>\n",
       "      <td>2.615500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280000</td>\n",
       "      <td>2.616200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1285000</td>\n",
       "      <td>2.616900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290000</td>\n",
       "      <td>2.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1295000</td>\n",
       "      <td>2.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300000</td>\n",
       "      <td>2.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305000</td>\n",
       "      <td>2.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310000</td>\n",
       "      <td>2.607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1315000</td>\n",
       "      <td>2.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320000</td>\n",
       "      <td>2.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325000</td>\n",
       "      <td>2.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330000</td>\n",
       "      <td>2.602300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1335000</td>\n",
       "      <td>2.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340000</td>\n",
       "      <td>2.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1345000</td>\n",
       "      <td>2.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350000</td>\n",
       "      <td>2.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1355000</td>\n",
       "      <td>2.593400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360000</td>\n",
       "      <td>2.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1365000</td>\n",
       "      <td>2.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370000</td>\n",
       "      <td>2.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375000</td>\n",
       "      <td>2.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380000</td>\n",
       "      <td>2.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1385000</td>\n",
       "      <td>2.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390000</td>\n",
       "      <td>2.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1395000</td>\n",
       "      <td>2.584900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400000</td>\n",
       "      <td>2.585300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1405000</td>\n",
       "      <td>2.580300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410000</td>\n",
       "      <td>2.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1415000</td>\n",
       "      <td>2.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420000</td>\n",
       "      <td>2.577700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425000</td>\n",
       "      <td>2.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430000</td>\n",
       "      <td>2.573400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1435000</td>\n",
       "      <td>2.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440000</td>\n",
       "      <td>2.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1445000</td>\n",
       "      <td>2.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450000</td>\n",
       "      <td>2.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455000</td>\n",
       "      <td>2.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460000</td>\n",
       "      <td>2.565300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1465000</td>\n",
       "      <td>2.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470000</td>\n",
       "      <td>2.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475000</td>\n",
       "      <td>2.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480000</td>\n",
       "      <td>2.561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1485000</td>\n",
       "      <td>2.560200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490000</td>\n",
       "      <td>2.563600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495000</td>\n",
       "      <td>2.559400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500000</td>\n",
       "      <td>2.556300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1505000</td>\n",
       "      <td>2.556600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510000</td>\n",
       "      <td>2.557200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1515000</td>\n",
       "      <td>2.554200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520000</td>\n",
       "      <td>2.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525000</td>\n",
       "      <td>2.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530000</td>\n",
       "      <td>2.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1535000</td>\n",
       "      <td>2.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540000</td>\n",
       "      <td>2.545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1545000</td>\n",
       "      <td>2.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550000</td>\n",
       "      <td>2.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1555000</td>\n",
       "      <td>2.536600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560000</td>\n",
       "      <td>2.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1565000</td>\n",
       "      <td>2.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570000</td>\n",
       "      <td>2.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575000</td>\n",
       "      <td>2.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580000</td>\n",
       "      <td>2.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1585000</td>\n",
       "      <td>2.533400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590000</td>\n",
       "      <td>2.533100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1595000</td>\n",
       "      <td>2.534800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600000</td>\n",
       "      <td>2.531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1605000</td>\n",
       "      <td>2.534800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610000</td>\n",
       "      <td>2.529200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1615000</td>\n",
       "      <td>2.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620000</td>\n",
       "      <td>2.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625000</td>\n",
       "      <td>2.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630000</td>\n",
       "      <td>2.525700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1635000</td>\n",
       "      <td>2.526400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640000</td>\n",
       "      <td>2.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1645000</td>\n",
       "      <td>2.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650000</td>\n",
       "      <td>2.519600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1655000</td>\n",
       "      <td>2.518200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660000</td>\n",
       "      <td>2.520500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1665000</td>\n",
       "      <td>2.517600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670000</td>\n",
       "      <td>2.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675000</td>\n",
       "      <td>2.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680000</td>\n",
       "      <td>2.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1685000</td>\n",
       "      <td>2.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690000</td>\n",
       "      <td>2.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1695000</td>\n",
       "      <td>2.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700000</td>\n",
       "      <td>2.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1705000</td>\n",
       "      <td>2.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710000</td>\n",
       "      <td>2.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1715000</td>\n",
       "      <td>2.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720000</td>\n",
       "      <td>2.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725000</td>\n",
       "      <td>2.506500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730000</td>\n",
       "      <td>2.506200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1735000</td>\n",
       "      <td>2.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740000</td>\n",
       "      <td>2.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1745000</td>\n",
       "      <td>2.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750000</td>\n",
       "      <td>2.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1755000</td>\n",
       "      <td>2.499800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760000</td>\n",
       "      <td>2.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1765000</td>\n",
       "      <td>2.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770000</td>\n",
       "      <td>2.495500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775000</td>\n",
       "      <td>2.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780000</td>\n",
       "      <td>2.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1785000</td>\n",
       "      <td>2.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790000</td>\n",
       "      <td>2.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1795000</td>\n",
       "      <td>2.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800000</td>\n",
       "      <td>2.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1805000</td>\n",
       "      <td>2.490900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810000</td>\n",
       "      <td>2.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1815000</td>\n",
       "      <td>2.486600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820000</td>\n",
       "      <td>2.489700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825000</td>\n",
       "      <td>2.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830000</td>\n",
       "      <td>2.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1835000</td>\n",
       "      <td>2.483700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840000</td>\n",
       "      <td>2.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1845000</td>\n",
       "      <td>2.478900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850000</td>\n",
       "      <td>2.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1855000</td>\n",
       "      <td>2.480400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860000</td>\n",
       "      <td>2.481500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1865000</td>\n",
       "      <td>2.480600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870000</td>\n",
       "      <td>2.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875000</td>\n",
       "      <td>2.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880000</td>\n",
       "      <td>2.472900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1885000</td>\n",
       "      <td>2.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890000</td>\n",
       "      <td>2.473500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1895000</td>\n",
       "      <td>2.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900000</td>\n",
       "      <td>2.476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1905000</td>\n",
       "      <td>2.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910000</td>\n",
       "      <td>2.473300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1915000</td>\n",
       "      <td>2.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920000</td>\n",
       "      <td>2.469900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925000</td>\n",
       "      <td>2.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930000</td>\n",
       "      <td>2.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1935000</td>\n",
       "      <td>2.470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940000</td>\n",
       "      <td>2.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1945000</td>\n",
       "      <td>2.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950000</td>\n",
       "      <td>2.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955000</td>\n",
       "      <td>2.466500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960000</td>\n",
       "      <td>2.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1965000</td>\n",
       "      <td>2.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970000</td>\n",
       "      <td>2.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975000</td>\n",
       "      <td>2.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980000</td>\n",
       "      <td>2.463800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1985000</td>\n",
       "      <td>2.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990000</td>\n",
       "      <td>2.465000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995000</td>\n",
       "      <td>2.463200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell-5\n",
    "#pretraining the model\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True,\n",
    "                                               mlm_probability=0.15)\n",
    "epochs = 2\n",
    "save_steps = 10000 #save checkpoint every 10000 steps\n",
    "batch_size = 128\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'bert_bbpe/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs = epochs,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    save_steps = save_steps,\n",
    "    save_total_limit = 5, #only save the last 5 checkpoints\n",
    "    fp16=True,\n",
    "    # tf32 = True,\n",
    "    learning_rate = 5e-5,  # 5e-5 is the default\n",
    "    logging_steps = 5_000,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    # gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    "\n",
    ")\n",
    "\n",
    "from pynvml import *\n",
    "\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "# trainer.train()\n",
    "trainer.save_model('bert_bbpe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319ddf3-b3da-463d-b864-475f07bc292d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff12a54-3b7f-4e72-8843-e2d196f4df26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c12f2-0702-4b4b-a622-5878ae280109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
